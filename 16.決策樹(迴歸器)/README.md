# 決策樹 (迴歸器)


## 今日學習目標
- 迴歸決策樹
    - 學習決策樹是如何處理連續性數值輸出
- 實作決策樹迴歸器
    - 查看決策樹方法在簡單線性回歸和非線性回歸表現


![](https://i.imgur.com/0E14xif.png)

決策樹回歸方法與分類有點類似差別僅在於評估分枝好壞的方式不同。當數據集的輸出爲連續性數值時，該樹算法就是一個迴歸樹。透過樹的展開，並用葉節點的均值作爲預測值。從根節點開始，對樣本的某一特徵進行測試。經過評估後，將樣本分配到其子結點。此時每一個子節點對應著該特徵的一個值。依照這樣方式進行，直至到達葉結點。此時誤差(loss)為0。

![](https://i.imgur.com/8R77SQ9.png)

## CART 決策樹
- scikit-learn 決策樹演算法採用 CART (Classification and Regression Tree) 樹演算法。
- 可以做分類和迴歸預測。
- 在每一個節點上都是採用二分法。

## 決策樹總結

決策樹透過所有特徵與對應的值將資料切分，來找出最適合的分枝並繼續往下拓展。若決策樹深度越深，則決策規則越複雜，模型也會越接近數據，但若數據中含有雜訊，太深的樹就有可能產生過擬合的情形。因此單一的回歸樹肯定是不夠用的。可以利用集成學習中的 Boosting 架構，對回歸樹進行改良升級。




本系列教學簡報 PDF & Code 都可以從我的 [GitHub](https://github.com/andy6804tw/2020-12th-ironman) 取得！